{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import section\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data = load_breast_cancer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "#examine data\n",
    "x_names = data['feature_names'] \n",
    "y_names = data['target_names'] \n",
    "x = data['data'] \n",
    "y = data['target']\n",
    "\n",
    "print(y_names)\n",
    "print(x_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200,), activation='logistic', alpha=0.001,\n",
    "                    solver='sgd', random_state=1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65870429\n",
      "Iteration 2, loss = 0.61957075\n",
      "Iteration 3, loss = 0.60035899\n",
      "Iteration 4, loss = 0.59402016\n",
      "Iteration 5, loss = 0.59005917\n",
      "Iteration 6, loss = 0.57546922\n",
      "Iteration 7, loss = 0.55881365\n",
      "Iteration 8, loss = 0.54050240\n",
      "Iteration 9, loss = 0.53116309\n",
      "Iteration 10, loss = 0.52590515\n",
      "Iteration 11, loss = 0.51438079\n",
      "Iteration 12, loss = 0.50903469\n",
      "Iteration 13, loss = 0.49478189\n",
      "Iteration 14, loss = 0.48631765\n",
      "Iteration 15, loss = 0.48133085\n",
      "Iteration 16, loss = 0.47281510\n",
      "Iteration 17, loss = 0.47183039\n",
      "Iteration 18, loss = 0.46364010\n",
      "Iteration 19, loss = 0.44966607\n",
      "Iteration 20, loss = 0.44496897\n",
      "Iteration 21, loss = 0.43807519\n",
      "Iteration 22, loss = 0.44452577\n",
      "Iteration 23, loss = 0.43327130\n",
      "Iteration 24, loss = 0.42265865\n",
      "Iteration 25, loss = 0.41901102\n",
      "Iteration 26, loss = 0.41391519\n",
      "Iteration 27, loss = 0.40737256\n",
      "Iteration 28, loss = 0.40276078\n",
      "Iteration 29, loss = 0.39738732\n",
      "Iteration 30, loss = 0.40279533\n",
      "Iteration 31, loss = 0.38955974\n",
      "Iteration 32, loss = 0.38948586\n",
      "Iteration 33, loss = 0.38580586\n",
      "Iteration 34, loss = 0.38509629\n",
      "Iteration 35, loss = 0.37761385\n",
      "Iteration 36, loss = 0.37443868\n",
      "Iteration 37, loss = 0.37225676\n",
      "Iteration 38, loss = 0.36621154\n",
      "Iteration 39, loss = 0.36051549\n",
      "Iteration 40, loss = 0.35439215\n",
      "Iteration 41, loss = 0.39510974\n",
      "Iteration 42, loss = 0.35334018\n",
      "Iteration 43, loss = 0.34647947\n",
      "Iteration 44, loss = 0.33968511\n",
      "Iteration 45, loss = 0.34001126\n",
      "Iteration 46, loss = 0.33816465\n",
      "Iteration 47, loss = 0.33020132\n",
      "Iteration 48, loss = 0.32963546\n",
      "Iteration 49, loss = 0.32412346\n",
      "Iteration 50, loss = 0.32433975\n",
      "Iteration 51, loss = 0.32801015\n",
      "Iteration 52, loss = 0.32879019\n",
      "Iteration 53, loss = 0.31855144\n",
      "Iteration 54, loss = 0.31192502\n",
      "Iteration 55, loss = 0.30715564\n",
      "Iteration 56, loss = 0.31205693\n",
      "Iteration 57, loss = 0.34322547\n",
      "Iteration 58, loss = 0.30446843\n",
      "Iteration 59, loss = 0.29813594\n",
      "Iteration 60, loss = 0.30318321\n",
      "Iteration 61, loss = 0.30287619\n",
      "Iteration 62, loss = 0.31374735\n",
      "Iteration 63, loss = 0.29124743\n",
      "Iteration 64, loss = 0.29637986\n",
      "Iteration 65, loss = 0.28843095\n",
      "Iteration 66, loss = 0.30014599\n",
      "Iteration 67, loss = 0.30615668\n",
      "Iteration 68, loss = 0.29008465\n",
      "Iteration 69, loss = 0.28327982\n",
      "Iteration 70, loss = 0.28005665\n",
      "Iteration 71, loss = 0.27799632\n",
      "Iteration 72, loss = 0.28819745\n",
      "Iteration 73, loss = 0.28103582\n",
      "Iteration 74, loss = 0.27540859\n",
      "Iteration 75, loss = 0.27540652\n",
      "Iteration 76, loss = 0.26937987\n",
      "Iteration 77, loss = 0.26841451\n",
      "Iteration 78, loss = 0.27355815\n",
      "Iteration 79, loss = 0.27507390\n",
      "Iteration 80, loss = 0.26652854\n",
      "Iteration 81, loss = 0.26507421\n",
      "Iteration 82, loss = 0.26612531\n",
      "Iteration 83, loss = 0.26791407\n",
      "Iteration 84, loss = 0.26851375\n",
      "Iteration 85, loss = 0.26256995\n",
      "Iteration 86, loss = 0.29039588\n",
      "Iteration 87, loss = 0.26608813\n",
      "Iteration 88, loss = 0.26066389\n",
      "Iteration 89, loss = 0.26159383\n",
      "Iteration 90, loss = 0.25751619\n",
      "Iteration 91, loss = 0.26530532\n",
      "Iteration 92, loss = 0.25763195\n",
      "Iteration 93, loss = 0.25370580\n",
      "Iteration 94, loss = 0.25247249\n",
      "Iteration 95, loss = 0.27063466\n",
      "Iteration 96, loss = 0.29132692\n",
      "Iteration 97, loss = 0.25435208\n",
      "Iteration 98, loss = 0.24978651\n",
      "Iteration 99, loss = 0.24799761\n",
      "Iteration 100, loss = 0.24550762\n",
      "Iteration 101, loss = 0.28451017\n",
      "Iteration 102, loss = 0.25305265\n",
      "Iteration 103, loss = 0.25499687\n",
      "Iteration 104, loss = 0.28188650\n",
      "Iteration 105, loss = 0.26457217\n",
      "Iteration 106, loss = 0.24287150\n",
      "Iteration 107, loss = 0.25401183\n",
      "Iteration 108, loss = 0.25379587\n",
      "Iteration 109, loss = 0.25747052\n",
      "Iteration 110, loss = 0.26408285\n",
      "Iteration 111, loss = 0.24797586\n",
      "Iteration 112, loss = 0.25968769\n",
      "Iteration 113, loss = 0.24076217\n",
      "Iteration 114, loss = 0.26423039\n",
      "Iteration 115, loss = 0.24874887\n",
      "Iteration 116, loss = 0.24957594\n",
      "Iteration 117, loss = 0.26478538\n",
      "Iteration 118, loss = 0.24837485\n",
      "Iteration 119, loss = 0.24174324\n",
      "Iteration 120, loss = 0.24755836\n",
      "Iteration 121, loss = 0.25492228\n",
      "Iteration 122, loss = 0.24211813\n",
      "Iteration 123, loss = 0.23555022\n",
      "Iteration 124, loss = 0.23575697\n",
      "Iteration 125, loss = 0.23402092\n",
      "Iteration 126, loss = 0.23076165\n",
      "Iteration 127, loss = 0.24471604\n",
      "Iteration 128, loss = 0.24328930\n",
      "Iteration 129, loss = 0.23367240\n",
      "Iteration 130, loss = 0.24012209\n",
      "Iteration 131, loss = 0.23846815\n",
      "Iteration 132, loss = 0.23614935\n",
      "Iteration 133, loss = 0.23487440\n",
      "Iteration 134, loss = 0.23454382\n",
      "Iteration 135, loss = 0.23676081\n",
      "Iteration 136, loss = 0.23141292\n",
      "Iteration 137, loss = 0.23932041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(200,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122807017543859"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
